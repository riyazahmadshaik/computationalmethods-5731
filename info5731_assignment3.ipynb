{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "info5731_assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyazahmadshaik/computationalmethods-5731/blob/master/info5731_assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "outputId": "ac6daebd-d713-4660-fa7e-b5f08aa7e4c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48xT-FdCicET",
        "outputId": "6c6930b5-e93e-4eed-cde5-dbe318dc8b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [39.3 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [370 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [57.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.4 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,353 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,748 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,685 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [213 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,115 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [239 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.9 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,165 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [863 kB]\n",
            "Fetched 11.2 MB in 4s (2,947 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 80.1 MB of archives.\n",
            "After this operation, 271 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 86.0.4240.75-0ubuntu0.18.04.1 [1,127 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 86.0.4240.75-0ubuntu0.18.04.1 [70.9 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 86.0.4240.75-0ubuntu0.18.04.1 [3,579 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 86.0.4240.75-0ubuntu0.18.04.1 [4,486 kB]\n",
            "Fetched 80.1 MB in 5s (15.1 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 144611 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_86.0.4240.75-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (86.0.4240.75-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_86.0.4240.75-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (86.0.4240.75-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_86.0.4240.75-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (86.0.4240.75-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_86.0.4240.75-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (86.0.4240.75-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (86.0.4240.75-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (86.0.4240.75-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (86.0.4240.75-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (86.0.4240.75-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: use options instead of chrome_options\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJhOvxqnjglS"
      },
      "source": [
        "#scraping the data from web to obtain the file\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from selenium.webdriver.common.by import By\n",
        "options = webdriver.ChromeOptions()\n",
        "url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "driver.get(url)\n",
        "time.sleep(10)\n",
        "Title = []\n",
        "date = []\n",
        "content= []\n",
        "page=0\n",
        "while True:\n",
        "    time.sleep(10)\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    results = soup.find_all(\"div\", {\"class\": \"review-container\"})\n",
        "    if(page==3) :\n",
        "      for result in results:\n",
        "          Title.append(result.find(\"a\", {\"class\": \"title\"}).get_text().strip())\n",
        "          date.append(result.find(\"span\", {\"review-date\"}).get_text().strip())\n",
        "          content.append(result.find(\"div\", {\"class\": \"content\"}).get_text().strip())\n",
        "    if len(driver.find_elements_by_css_selector('.load-more-data')) > 0:\n",
        "          driver.find_element_by_css_selector('.load-more-data').click()\n",
        "          page+=1\n",
        "          if int(page)>3:\n",
        "           break\n",
        "    else:\n",
        "          break\n",
        "df = pd.DataFrame({\"Title\": Title, \"date\": date, \"content\": content})\n",
        "df.to_csv(\"output_review.csv\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMm28oXaj7X5"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "pd.read_csv('output_review.csv', encoding='utf-8')\n",
        "df = pd.read_csv('output_review.csv')\n",
        "\n",
        "df.columns = ['','Title', 'date', 'content']\n",
        "#lowercase and removing special characters and punctuation marks\n",
        "df['Title'] = df['Title'].str.lower()\n",
        "df['content'] = df['content'].str.lower()\n",
        "df['Title']= df['Title'].str.replace(r'[^\\w\\s]+','')\n",
        "df['content']= df['content'].str.replace(r'[^\\w\\s]+','')\n",
        "#removing numbers\n",
        "df['Title'] = df['Title'].str.replace('\\d+', '')\n",
        "df['content'] = df['content'].str.replace('\\d+', '')\n",
        "#removing stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['Title'] = df['Title'].apply(lambda x: [item for item in str.split(x) if item not in stop])\n",
        "df['content'] = df['content'].apply(lambda x: [item for item in str.split(x) if item not in stop])\n",
        "#lemmatization\n",
        "df['Title']= df['Title'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n",
        "#df['Title'] = df['Title'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "df['content']= df['content'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n",
        "#stemming\n",
        "df['Title']= df['Title'].apply(lambda x : [ps.stem(y) for y in x])\n",
        "df['content']= df['content'].apply(lambda x : [ps.stem(y) for y in x])\n",
        "\n",
        "def remove_punc(text):\n",
        "  no_punc=\" \".join([c for c in text if c not in string.punctuation])\n",
        "  return no_punc\n",
        "df['Title']= df['Title'].apply(lambda x : remove_punc(x))\n",
        "df['content']= df['content'].apply(lambda x : remove_punc(x))\n",
        "\n",
        "#Adding columns for clean_content and clean_title in a csv\n",
        "csv_input = pd.read_csv('output_review.csv')\n",
        "csv_input['clean_title'] = df['Title']\n",
        "csv_input['clean_content'] = df['content']\n",
        "csv_input.to_csv('output_review.csv', index=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpYbxnB1kEe6",
        "outputId": "d65510fe-9c20-452c-b969-1c7fa2fa45fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "data = pd.read_csv('output_review.csv', low_memory=False)\n",
        "word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(data['clean_title'].values.astype('U')) \n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "df"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>account rate disappoint</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>account tell everyon</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>act cannot save</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>act perform ive</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>act perform phoenix</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worth watch joker</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worthi act perform</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>would call masterpiec</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>write review that</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>yike peopl best</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>179 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Frequency\n",
              "account rate disappoint          1\n",
              "account tell everyon             1\n",
              "act cannot save                  1\n",
              "act perform ive                  1\n",
              "act perform phoenix              1\n",
              "...                            ...\n",
              "worth watch joker                1\n",
              "worthi act perform               1\n",
              "would call masterpiec            1\n",
              "write review that                1\n",
              "yike peopl best                  1\n",
              "\n",
              "[179 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XxVBgG8kMzz"
      },
      "source": [
        "import csv\n",
        "with open('output_review.csv') as f:\n",
        " reader = csv.reader(f)\n",
        " next(reader, None)\n",
        " clean_title = [row[4] for row in reader]\n",
        "with open('clean_title.txt', mode=\"w\") as outfile:\n",
        "    for s in clean_title:\n",
        "        outfile.write(\"%s\\n\" % s)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcrSkvUxkPyQ"
      },
      "source": [
        "def bigramEstimation(file):\n",
        "    lst = []\n",
        "    unigrams = {}\n",
        "    bigrams = {} \n",
        "    text = open(file, 'r').read()\n",
        "    lst = text.strip().split()\n",
        "    del text \n",
        "    for l in lst:\n",
        "        if not l in unigrams:\n",
        "            unigrams[l] = 1\n",
        "        else:\n",
        "            unigrams[l] += 1\n",
        "    for i in range(len(lst) - 1):\n",
        "        temp = (lst[i], lst[i+1])\n",
        "        if not temp in bigrams:\n",
        "            bigrams[temp] = 1\n",
        "        else:\n",
        "          bigrams[temp] += 1\n",
        "    print('Generated ', len(bigrams), ' bigrams')\n",
        "    total_corpus = sum(unigrams.values())\n",
        "    for k,v in bigrams.items():\n",
        "        first_word = k[0]\n",
        "        first_word_count = unigrams[first_word]\n",
        "        bi_prob = bigrams[k] / unigrams[first_word]\n",
        "        if(v == 2):\n",
        "         print(k[0] ,k[1],v ,bi_prob)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N6B_SICkVV1",
        "outputId": "20c0b25f-6fce-4bbe-9145-a46dc52a7929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "bigramEstimation('clean_title.txt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated  344  bigrams\n",
            "amaz movi 2 0.3333333333333333\n",
            "dark knight 2 0.4\n",
            "one best 2 1.0\n",
            "act perform 2 0.4\n",
            "ive ever 2 1.0\n",
            "extrem overr 2 1.0\n",
            "believ hype 2 1.0\n",
            "best movi 2 0.16666666666666666\n",
            "perform phoenix 2 0.5\n",
            "best comic 2 0.16666666666666666\n",
            "comic book 2 1.0\n",
            "rise joker 2 1.0\n",
            "dc movi 2 1.0\n",
            "dark movi 2 0.4\n",
            "movi amaz 2 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4jx6Qyqkic7",
        "outputId": "c3ce11e8-63f5-4324-ec63-34f6f67b378a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.3.1)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEqKvk82kkPR",
        "outputId": "80a317b8-e766-4d3d-de28-0eb42a8236ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import spacy\n",
        "import csv\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "file = open(\"clean_title.txt\", \"r\")\n",
        "doc = nlp(file.read())\n",
        "noun_phrases = []\n",
        "for np in doc.noun_chunks:\n",
        "  noun_phrases.append(np.text)\n",
        "print(noun_phrases)\n",
        "dfn = pd.DataFrame(noun_phrases, columns = ['noun_phrases'])\n",
        "word_vectorizer = CountVectorizer(ngram_range=(2,7), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(dfn['noun_phrases'].values.astype('U')) \n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "dff = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "dff['Noun_Probabilities'] = dff['Frequency'] / dff['Frequency'].max()\n",
        "result = dff.astype(object).transpose() \n",
        "#print(tabulate(result, headers='keys', tablefmt='grid'))\n",
        "result"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['outstand movi haunt', 'best charact', 'certain peopl relat\\nperfect everi aspect', 'masterpiec', 'hype', 'amaz movi', 'psycholog studi', 'rather superhero flick', 'joaquin oscar joker', 'best dark suspens thriller darker dark knight', 'venic review', 'masterpiec', 'final real movi', 'spoon feed cgi fuel faux drama', 'good lord', 'joker endgam', 'oscar', 'phoenix', 'critic', 'one best act', 'i', 'extrem overr', 'ok film', 'believ hype', 'yike', 'extrem overr', 'absolut phenomen', 'best film', 'st centuri', 'masterpiec', 'that life', 'old peopl', 'worthi act', 'phoenix', 'watch', 'joker', 'enough becom joker', 'we', 'miser unpleas slog movi noth', 'broken man', 'brilliant best joker', 'i', 'movi', 'clown princ crime arriv', 'probabl', 'one best comic book movi', 'phoenix', 'speechless end', 'everyon brain wash', 'account rate disappoint film', 'nonsens plot', 'rise joker', 'stun', 'account', 'good\\nrepresint real life', 'joker', 'anyon rate movi poorli clearli', 'cinema', 'overratedoverhyp', 'astonish masterpiec', 'who', 'hour', 'full bad bad mood', 'instant cult movi', 'joke', 'compar endgam', 'dont believ hype', 'joketh movi', 'kind letdown', 'garbag hype', 'dc movi', 'best dc movi sinc dark knight', 'joker', 'overhyp', 'proper entertain millenni joker', 'beauti', 'masterpiec', 'everyon', 'dark movi', 'damn oscar joaquin', 'joker\\nfantast', 'joaquin phoenix', 'good movi', 'amaz', 'dark movi', 'amaz film', 'big punch wonder act', 'absolut amaz', 'import difficult beauti artist film', 'mess interest\\nrate', 'ban masterpiec', 'oscar', 'mesmer\\ninsult best comic book', 'villain time', 'peopl', 'bore', 'mani cheap actionmodern superhero genr movi', 'everyon els', 'best genr', 'alltim', 'favorit film', 'grandmast class', 'stun', 'overr badli direct film', 'mislead titl', 'overr bore cheap pretenti movi', 'dumb peopl', 'best movi year', 'great act', 'terribl film', 'review', 'good\\nact', 'mediocr script', 'joke movi', 'good film', 'noth joker', 'realli', 'amaz movi']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>absolut amaz</th>\n",
              "      <th>absolut phenomen</th>\n",
              "      <th>account rate</th>\n",
              "      <th>account rate disappoint</th>\n",
              "      <th>account rate disappoint film</th>\n",
              "      <th>actionmodern superhero</th>\n",
              "      <th>actionmodern superhero genr</th>\n",
              "      <th>actionmodern superhero genr movi</th>\n",
              "      <th>amaz film</th>\n",
              "      <th>amaz movi</th>\n",
              "      <th>anyon rate</th>\n",
              "      <th>anyon rate movi</th>\n",
              "      <th>anyon rate movi poorli</th>\n",
              "      <th>anyon rate movi poorli clearli</th>\n",
              "      <th>artist film</th>\n",
              "      <th>astonish masterpiec</th>\n",
              "      <th>bad bad</th>\n",
              "      <th>bad bad mood</th>\n",
              "      <th>bad mood</th>\n",
              "      <th>badli direct</th>\n",
              "      <th>badli direct film</th>\n",
              "      <th>ban masterpiec</th>\n",
              "      <th>beauti artist</th>\n",
              "      <th>beauti artist film</th>\n",
              "      <th>becom joker</th>\n",
              "      <th>believ hype</th>\n",
              "      <th>best act</th>\n",
              "      <th>best charact</th>\n",
              "      <th>best comic</th>\n",
              "      <th>best comic book</th>\n",
              "      <th>best comic book movi</th>\n",
              "      <th>best dark</th>\n",
              "      <th>best dark suspens</th>\n",
              "      <th>best dark suspens thriller</th>\n",
              "      <th>best dark suspens thriller darker</th>\n",
              "      <th>best dark suspens thriller darker dark</th>\n",
              "      <th>best dark suspens thriller darker dark knight</th>\n",
              "      <th>best dc</th>\n",
              "      <th>best dc movi</th>\n",
              "      <th>best dc movi sinc</th>\n",
              "      <th>...</th>\n",
              "      <th>rather superhero</th>\n",
              "      <th>rather superhero flick</th>\n",
              "      <th>real life</th>\n",
              "      <th>real movi</th>\n",
              "      <th>relat perfect</th>\n",
              "      <th>relat perfect everi</th>\n",
              "      <th>relat perfect everi aspect</th>\n",
              "      <th>represint real</th>\n",
              "      <th>represint real life</th>\n",
              "      <th>rise joker</th>\n",
              "      <th>sinc dark</th>\n",
              "      <th>sinc dark knight</th>\n",
              "      <th>slog movi</th>\n",
              "      <th>slog movi noth</th>\n",
              "      <th>speechless end</th>\n",
              "      <th>spoon feed</th>\n",
              "      <th>spoon feed cgi</th>\n",
              "      <th>spoon feed cgi fuel</th>\n",
              "      <th>spoon feed cgi fuel faux</th>\n",
              "      <th>spoon feed cgi fuel faux drama</th>\n",
              "      <th>st centuri</th>\n",
              "      <th>superhero flick</th>\n",
              "      <th>superhero genr</th>\n",
              "      <th>superhero genr movi</th>\n",
              "      <th>suspens thriller</th>\n",
              "      <th>suspens thriller darker</th>\n",
              "      <th>suspens thriller darker dark</th>\n",
              "      <th>suspens thriller darker dark knight</th>\n",
              "      <th>terribl film</th>\n",
              "      <th>that life</th>\n",
              "      <th>thriller darker</th>\n",
              "      <th>thriller darker dark</th>\n",
              "      <th>thriller darker dark knight</th>\n",
              "      <th>unpleas slog</th>\n",
              "      <th>unpleas slog movi</th>\n",
              "      <th>unpleas slog movi noth</th>\n",
              "      <th>venic review</th>\n",
              "      <th>villain time</th>\n",
              "      <th>wonder act</th>\n",
              "      <th>worthi act</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Frequency</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Noun_Probabilities</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 261 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   absolut amaz absolut phenomen  ... wonder act worthi act\n",
              "Frequency                     1                1  ...          1          1\n",
              "Noun_Probabilities          0.5              0.5  ...        0.5        0.5\n",
              "\n",
              "[2 rows x 261 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "outputId": "dedaa100-d3d3-4da7-c81b-63f5c6dfdc8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"output_review.csv\")\n",
        "tf2 = df.dropna()\n",
        "tf1 = (tf2['clean_title'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index())\n",
        "tf1.columns = ['words','tf']\n",
        "tf1\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>seen</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>movi</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>charact</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>haunt</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>best</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>mediocr</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>cannot</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>save</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>understand</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>realli</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>205 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          words    tf\n",
              "0          seen   3.0\n",
              "1          movi  20.0\n",
              "2       charact   2.0\n",
              "3         haunt   1.0\n",
              "4          best  12.0\n",
              "..          ...   ...\n",
              "200     mediocr   1.0\n",
              "201      cannot   1.0\n",
              "202        save   1.0\n",
              "203  understand   1.0\n",
              "204      realli   1.0\n",
              "\n",
              "[205 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO6fWzzGkzVi",
        "outputId": "fe52926b-8233-492a-f322-8fa5c276ec52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import numpy as np\n",
        "for i,word in enumerate(tf1['words']):\n",
        "  tf1.loc[i, 'idf'] = np.log(df.shape[0]/(len(tf2[tf2['clean_title'].str.contains(word)])))\n",
        "tf1['tf*idf'] = tf1['tf'] * tf1['idf']\n",
        "tf1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tf*idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>seen</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.506558</td>\n",
              "      <td>10.519674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>movi</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>32.188758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>charact</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.912023</td>\n",
              "      <td>7.824046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>haunt</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>best</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.120264</td>\n",
              "      <td>25.443162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>mediocr</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>cannot</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>save</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>understand</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>realli</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>205 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          words    tf       idf     tf*idf\n",
              "0          seen   3.0  3.506558  10.519674\n",
              "1          movi  20.0  1.609438  32.188758\n",
              "2       charact   2.0  3.912023   7.824046\n",
              "3         haunt   1.0  4.605170   4.605170\n",
              "4          best  12.0  2.120264  25.443162\n",
              "..          ...   ...       ...        ...\n",
              "200     mediocr   1.0  4.605170   4.605170\n",
              "201      cannot   1.0  4.605170   4.605170\n",
              "202        save   1.0  4.605170   4.605170\n",
              "203  understand   1.0  4.605170   4.605170\n",
              "204      realli   1.0  4.605170   4.605170\n",
              "\n",
              "[205 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn1Ly6Hkk5Dt"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import numpy.linalg as LA\n",
        "doc_set = tf2['clean_title'].values.tolist()\n",
        "query_set = \"As a viewer that actually went to TIFF and witnessed this film and didn't want to believe the hype, it is an absolute MASTERPIECE and Phoenix is a certified legend.\"\n",
        "query_set = [query_set]\n",
        "stopWords = stopwords.words('english')\n",
        "vectorizer = CountVectorizer(stop_words = stopWords)\n",
        "transformer = TfidfTransformer()\n",
        "docVectorizerArray = vectorizer.fit_transform(doc_set).toarray()\n",
        "queryVectorizerArray = vectorizer.transform(query_set).toarray()\n",
        "cx = lambda a, b : np.inner(a, b)/(LA.norm(a)*LA.norm(b))\n",
        "result = []\n",
        "for vector in docVectorizerArray:\n",
        "        for testV in queryVectorizerArray:\n",
        "            cosine = cx(vector, testV)\n",
        "            result.append(cosine)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVGPk1MUk9WV",
        "outputId": "cf81afe2-e828-41be-ce59-acc4d464c4c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "new = tf2.filter(['Unnamed','clean_title'], axis=1)\n",
        "se = pd.Series(result)\n",
        "new['Cosine_similarity'] = se.values\n",
        "new.drop(new.loc[new['Cosine_similarity']==0].index, inplace=True)\n",
        "new[\"Rank\"] = new[\"Cosine_similarity\"].rank().astype(int)\n",
        "new.sort_values(\"Cosine_similarity\", inplace = True) \n",
        "new"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_title</th>\n",
              "      <th>Cosine_similarity</th>\n",
              "      <th>Rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>worthi act perform phoenix worth watch joker s...</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>probabl one best comic book movi perform phoen...</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>overr badli direct film mislead titl</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>import difficult beauti artist film watch</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>made account rate disappoint film</td>\n",
              "      <td>0.223607</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>joaquin phoenix best work</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>great act terribl film</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>good film noth joker</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>went second time watch</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>best film st centuri</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>dont believ hype</td>\n",
              "      <td>0.288675</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>alltim favorit film</td>\n",
              "      <td>0.288675</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>believ hype</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>garbag hype</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>ok film</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>amaz film</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>oscar phoenix</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hype real</td>\n",
              "      <td>0.353553</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_title  Cosine_similarity  Rank\n",
              "31  worthi act perform phoenix worth watch joker s...           0.125000     1\n",
              "37  probabl one best comic book movi perform phoen...           0.144338     2\n",
              "89               overr badli direct film mislead titl           0.204124     3\n",
              "77          import difficult beauti artist film watch           0.204124     3\n",
              "39                  made account rate disappoint film           0.223607     5\n",
              "70                          joaquin phoenix best work           0.250000     8\n",
              "92                             great act terribl film           0.250000     8\n",
              "97                               good film noth joker           0.250000     8\n",
              "6                              went second time watch           0.250000     8\n",
              "26                               best film st centuri           0.250000     8\n",
              "54                                   dont believ hype           0.288675    11\n",
              "86                                alltim favorit film           0.288675    11\n",
              "22                                        believ hype           0.353553    15\n",
              "57                                        garbag hype           0.353553    15\n",
              "20                                            ok film           0.353553    15\n",
              "73                                          amaz film           0.353553    15\n",
              "15                                      oscar phoenix           0.353553    15\n",
              "4                                           hype real           0.353553    15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: https://github.com/riyazahmadshaik/computationalmethods-5731/blob/master/Infoassignment3.csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}