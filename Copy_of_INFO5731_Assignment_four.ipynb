{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_four.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyazahmadshaik/computationalmethods-5731/blob/master/Copy_of_INFO5731_Assignment_four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis, and regression analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you understand topic modeling better as well as how to visualize topic modeling results, aims to collect the human meanings of documents. Based on the yelp review data (only the review text will be used for this question), which can be download from Dropbox: https://www.dropbox.com/s/59hsrk56sfwh9u2/Assignment%20four%20data%20Yelp%20%28question%201%20and%202%29.zip?dl=0, **select two models** and write a python program to **identify the top 20 topics (with 15 words for each topic) in the dataset**. Before answering this question, please review the materials in lesson 8, as well as the introduction of these models by the links provided.\n",
        "\n",
        "(1)   Labeled LDA (LLDA): https://github.com/JoeZJH/Labeled-LDA-Python\n",
        "\n",
        "(2)   Biterm Topic Model (BTM): https://github.com/markoarnauto/biterm\n",
        "\n",
        "(3)   HMM-LDA: https://github.com/dongwookim-ml/python-topic-model\n",
        "\n",
        "(4)   SupervisedLDA: https://github.com/dongwookim-ml/python-topic-model/tree/master/notebook\n",
        "\n",
        "(5)   Relational Topic Model: https://github.com/dongwookim-ml/python-topic-model/tree/master/notebook\n",
        "\n",
        "(6)   LDA2VEC: https://github.com/cemoody/lda2vec\n",
        "\n",
        "(7)   BERTopic: https://github.com/MaartenGr/BERTopic\n",
        "\n",
        "(8)   LDA+BERT Topic Modeling: https://www.kaggle.com/dskswu/topic-modeling-bert-lda\n",
        "\n",
        "(9)   Clustering for Topic models: (paper: https://arxiv.org/abs/2004.14914), (code: https://github.com/adalmia96/Cluster-Analysis)\n",
        "\n",
        "\n",
        "**The following information should be reported:**\n",
        "\n",
        "(1) Top 20 clusters for topic modeling.\n",
        "\n",
        "(2) Summarize and describe the topic for each cluster. \n",
        "\n",
        "(3) Visualize the topic modeling reasults by using pyLDAVis: https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/#14.-pyLDAVis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "983b6da8-e4d7-4fcf-bf11-20075dcc30dd"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "papers = pd.read_csv(\"Assignment4_File_5731.csv\",encoding = \"ISO-8859-1\")\n",
        "papers_data = papers['Clean_Title']\n",
        "papers.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document_ID</th>\n",
              "      <th>Clean_Title</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>viewer actual went tiff wit film didnt want be...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>outstand movi haunt perform best charact devel...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>certain peopl relat</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>perfect everi aspect</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>hype real</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Document_ID                                        Clean_Title Sentiment\n",
              "0            1  viewer actual went tiff wit film didnt want be...  Positive\n",
              "1            2  outstand movi haunt perform best charact devel...  Positive\n",
              "2            3                                certain peopl relat   Neutral\n",
              "3            4                               perfect everi aspect  Positive\n",
              "4            5                                          hype real  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKW8_qpQqaN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd27c55-2c58-4215-b581-642694e79d38"
      },
      "source": [
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "data = papers.Clean_Title.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['viewer', 'actual', 'went', 'tiff', 'wit', 'film', 'didnt', 'want', 'believ', 'hype', 'absolut', 'masterpiec', 'phoenix', 'certifi', 'legend'], ['outstand', 'movi', 'haunt', 'perform', 'best', 'charact', 'develop', 'ever', 'seen'], ['certain', 'peopl', 'relat'], ['perfect', 'everi', 'aspect'], ['hype', 'real'], ['masterpiec'], ['amaz', 'movi', 'exist'], ['went', 'second', 'time', 'watch'], ['psycholog', 'studi', 'rather', 'superhero', 'flick'], ['joaquin', 'oscar', 'joker', 'best', 'dark', 'suspens', 'thriller', 'darker', 'dark', 'knight'], ['venic', 'review'], ['represint', 'real', 'life', 'joker'], ['final', 'real', 'movi'], ['good', 'lord'], ['spoon', 'feed', 'cgi', 'fuel', 'faux', 'drama'], ['oscar', 'phoenix'], ['critic', 'useless'], ['joker', 'endgam'], ['would', 'call', 'masterpiec'], ['one', 'best', 'act', 'perform', 'ive', 'ever', 'seen'], ['dont', 'forget', 'smile'], ['ok', 'film'], ['extrem', 'overr'], ['believ', 'hype'], ['masterpiec'], ['great', 'dont', 'know'], ['that', 'life'], ['extrem', 'overr'], ['requiem', 'broken', 'man'], ['rise', 'joker'], ['brilliant', 'best', 'joker', 'ive', 'ever', 'seen'], ['movi', 'decad'], ['omg', 'get', 'old', 'peopl', 'get', 'stupid'], ['clown', 'princ', 'crime', 'arriv'], ['probabl', 'one', 'best', 'comic', 'book', 'movi', 'perform', 'phoenix', 'make', 'speechless', 'end', 'venic'], ['worthi', 'act', 'perform', 'phoenix', 'worth', 'watch', 'joker', 'smart', 'enough', 'becom', 'joker', 'weve', 'come', 'know'], ['saw', 'tonight', 'make', 'account', 'tell', 'everyon', 'good'], ['miser', 'unpleas', 'slog', 'movi', 'noth'], ['stun'], ['anyon', 'rate', 'movi', 'poorli', 'clearli', 'doesnt', 'appreci', 'cinema'], ['bewar', 'anyon', 'hail', 'masterpiec'], ['yike', 'peopl', 'best', 'movi', 'ever'], ['dont', 'get'], ['astonish', 'masterpiec'], ['feel', 'like', 'everyon', 'brain', 'wash'], ['nonsens', 'plot'], ['made', 'account', 'rate', 'disappoint', 'film'], ['stop', 'compar', 'endgam'], ['dc', 'movi'], ['hour', 'full', 'bad', 'bad', 'mood'], ['who', 'real', 'joker'], ['best', 'dc', 'movi', 'sinc', 'dark', 'knight', 'rise'], ['joke', 'movi'], ['kind', 'letdown'], ['best', 'movi', 'year'], ['dont', 'believ', 'hype'], ['garbag', 'hype'], ['oscar', 'sooner', 'later'], [], ['joke'], ['perfect'], ['first', 'time', 'write', 'review', 'that', 'good'], ['mesmer'], ['dont', 'sheep'], ['overr', 'badli', 'direct', 'film', 'mislead', 'titl'], ['overhyp', 'overact', 'proper', 'entertain', 'millenni', 'joker'], ['dont', 'get', 'everyon', 'els', 'seem', 'see'], ['insult', 'best', 'comic', 'book', 'villain', 'time'], ['joker', 'laughter', 'joke', 'matter'], ['masterpiec'], ['best', 'thing', 'dc', 'done', 'sinc', 'dark', 'knight'], ['amaz', 'movi'], ['overr', 'bore', 'cheap', 'pretenti', 'movi', 'make', 'dumb', 'peopl', 'feel', 'smarter'], ['dark', 'seriou', 'tens', 'bad'], ['spectacular', 'awesom', 'fantast'], ['gener'], ['good', 'film', 'noth', 'joker'], ['great', 'act', 'terribl', 'film'], ['overhyp'], ['clear', 'shelf', 'oscar'], ['masterpiec', 'movi', 'year', 'best', 'actor', 'joaquin', 'phoenix'], ['hat', 'joaquin', 'phoenix'], ['real', 'joker'], ['fell', 'asleep'], ['realli', 'dont', 'understand'], ['went', 'blind', 'didnt', 'enjoy', 'realli', 'disappoint'], ['cinemat', 'masterpiec'], ['joker', 'outsid', 'crimin'], ['overact', 'galor'], ['uncomfort', 'satisfi'], ['far', 'best', 'dc', 'movi', 'date', 'joaquin', 'phoenix', 'perfect'], ['joaquin', 'phoenix', 'deliv', 'stori', 'doesn'], ['dark', 'depress'], ['overhyp', 'movi'], ['overr', 'overhyp'], ['masterpiec'], ['joker'], ['deriv', 'uninspir']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fig8uYx5qf4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3916f41a-8911-4da5-f872-79be549765dd"
      },
      "source": [
        "\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHv_ltL8qoFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4e8231-761c-4de8-b043-a99f0be86ad0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdkFkxTCqxCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3aa351d-c8e8-432c-b915-b6bab7ecf513"
      },
      "source": [
        "import spacy\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "print(data_lemmatized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['go', 'want', 'believ', 'hype'], ['perform', 'good', 'charact', 'develop', 'ever', 'see'], ['certain', 'peopl', 'relat'], ['perfect', 'everi', 'aspect'], ['hype'], [], ['exist'], ['go', 'second', 'time', 'watch'], ['rather', 'superhero'], ['good', 'dark', 'suspen'], ['review'], ['real', 'life', 'joker'], ['final', 'real', 'movi'], ['good'], ['feed', 'cgi', 'fuel', 'faux', 'drama'], [], ['critic', 'useless'], [], ['would', 'call'], ['good', 'act', 'perform', 'have', 'ever', 'see'], ['forget', 'smile'], ['film'], ['extrem', 'overr'], [], [], ['great', 'know'], ['life'], ['extrem', 'overr'], ['requiem', 'broken', 'man'], ['rise'], ['brilliant', 'best', 'joker', 'have', 'ever', 'see'], [], ['old', 'peopl', 'get', 'stupid'], [], ['probabl', 'good', 'comic', 'book', 'perform', 'make', 'speechless', 'end', 'venic'], ['perform', 'have', 'come', 'know'], ['see', 'tonight', 'make', 'account', 'tell', 'everyon', 'good'], [], [], ['anyon', 'rate', 'cinema'], [], ['best', 'movi', 'ever'], [], ['astonish'], ['feel', 'everyon', 'brain', 'wash'], [], ['make', 'account', 'rate', 'disappoint', 'film'], ['stop', 'compar', 'endgam'], [], ['hour', 'full', 'bad', 'bad', 'mood'], ['real'], ['good'], ['joke'], [], ['good', 'year'], ['believ', 'hype'], [], ['sooner', 'later'], [], ['joke'], ['perfect'], ['first', 'time', 'write', 'review', 'good'], [], ['sheep'], ['film'], [], ['everyon', 'seem'], ['good', 'comic', 'book', 'time'], [], [], ['good', 'thing'], [], ['cheap', 'pretenti', 'movi', 'make', 'dumb', 'peopl', 'feel', 'smart'], ['ten', 'bad'], [], [], ['good'], [], [], [], ['good', 'actor'], [], ['real'], ['fall', 'asleep'], ['realli', 'understand'], ['go', 'blind', 'enjoy', 'realli', 'disappoint'], ['cinemat'], [], [], [], ['far', 'good'], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqBVkYNGq3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "189337a6-3b41-4893-cfb3-17b0fd7c3f27"
      },
      "source": [
        "import gensim.corpora as corpora\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1)], [(13, 1), (14, 1), (15, 1)], [(2, 1)], [], [(16, 1)], [(1, 1), (17, 1), (18, 1), (19, 1)], [(20, 1), (21, 1)], [(7, 1), (22, 1), (23, 1)], [(24, 1)], [(25, 1), (26, 1), (27, 1)], [(27, 1), (28, 1), (29, 1)], [(7, 1)], [(30, 1), (31, 1), (32, 1), (33, 1), (34, 1)], [], [(35, 1), (36, 1)], [], [(37, 1), (38, 1)], [(6, 1), (7, 1), (8, 1), (9, 1), (39, 1), (40, 1)], [(41, 1), (42, 1)], [(43, 1)], [(44, 1), (45, 1)], [], [], [(46, 1), (47, 1)], [(26, 1)], [(44, 1), (45, 1)], [(48, 1), (49, 1), (50, 1)], [(51, 1)], [(6, 1), (9, 1), (25, 1), (40, 1), (52, 1), (53, 1)], [], [(11, 1), (54, 1), (55, 1), (56, 1)], [], [(7, 1), (8, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1)], [(8, 1), (40, 1), (47, 1), (64, 1)], [(7, 1), (9, 1), (60, 1), (65, 1), (66, 1), (67, 1), (68, 1)], [], [], [(69, 1), (70, 1), (71, 1)], [], [(6, 1), (29, 1), (52, 1)], [], [(72, 1)], [(66, 1), (73, 1), (74, 1), (75, 1)], [], [(43, 1), (60, 1), (65, 1), (71, 1), (76, 1)], [(77, 1), (78, 1), (79, 1)], [], [(80, 2), (81, 1), (82, 1), (83, 1)], [(27, 1)], [(7, 1)], [(84, 1)], [], [(7, 1), (85, 1)], [(0, 1), (2, 1)], [], [(86, 1), (87, 1)], [], [(84, 1)], [(15, 1)], [(7, 1), (18, 1), (24, 1), (88, 1), (89, 1)], [], [(90, 1)], [(43, 1)], [], [(66, 1), (91, 1)], [(7, 1), (18, 1), (57, 1), (58, 1)], [], [], [(7, 1), (92, 1)], [], [(11, 1), (29, 1), (60, 1), (74, 1), (93, 1), (94, 1), (95, 1), (96, 1)], [(80, 1), (97, 1)], [], [], [(7, 1)], [], [], [], [(7, 1), (98, 1)], [], [(27, 1)], [(99, 1), (100, 1)], [(101, 1), (102, 1)], [(1, 1), (76, 1), (101, 1), (103, 1), (104, 1)], [(105, 1)], [], [], [], [(7, 1), (106, 1)], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6C1PEA-q71j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692b9847-56a6-4ceb-fd31-843ff299ed3d"
      },
      "source": [
        "from pprint import pprint\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=20, \n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=20,\n",
        "                                       per_word_topics=True)\n",
        "pprint(lda_model.print_topics(num_words=4))\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.490*\"good\" + 0.073*\"thing\" + 0.073*\"astonish\" + 0.003*\"far\"'),\n",
            " (1, '0.125*\"good\" + 0.064*\"first\" + 0.064*\"act\" + 0.064*\"see\"'),\n",
            " (2, '0.143*\"rather\" + 0.143*\"superhero\" + 0.007*\"brain\" + 0.007*\"rate\"'),\n",
            " (3, '0.154*\"bad\" + 0.079*\"suspen\" + 0.079*\"hour\" + 0.079*\"good\"'),\n",
            " (4, '0.143*\"extrem\" + 0.143*\"overr\" + 0.073*\"sooner\" + 0.073*\"rate\"'),\n",
            " (5, '0.125*\"joke\" + 0.064*\"broken\" + 0.064*\"requiem\" + 0.064*\"peopl\"'),\n",
            " (6, '0.126*\"everyon\" + 0.126*\"cinemat\" + 0.126*\"seem\" + 0.006*\"rate\"'),\n",
            " (7, '0.165*\"review\" + 0.008*\"astonish\" + 0.008*\"feel\" + 0.008*\"rate\"'),\n",
            " (8, '0.112*\"actor\" + 0.112*\"final\" + 0.112*\"real\" + 0.112*\"movi\"'),\n",
            " (9, '0.247*\"hype\" + 0.166*\"believ\" + 0.085*\"want\" + 0.085*\"go\"'),\n",
            " (10, '0.199*\"real\" + 0.133*\"life\" + 0.068*\"movi\" + 0.068*\"joker\"'),\n",
            " (11, '0.102*\"good\" + 0.101*\"book\" + 0.101*\"sheep\" + 0.101*\"comic\"'),\n",
            " (12, '0.073*\"feed\" + 0.073*\"faux\" + 0.073*\"fuel\" + 0.073*\"would\"'),\n",
            " (13, '0.101*\"exist\" + 0.101*\"ten\" + 0.101*\"asleep\" + 0.101*\"fall\"'),\n",
            " (14, '0.112*\"everyon\" + 0.112*\"feel\" + 0.112*\"brain\" + 0.112*\"wash\"'),\n",
            " (15, '0.084*\"realli\" + 0.043*\"understand\" + 0.043*\"go\" + 0.043*\"perfect\"'),\n",
            " (16, '0.070*\"see\" + 0.070*\"make\" + 0.070*\"account\" + 0.036*\"tonight\"'),\n",
            " (17, '0.085*\"watch\" + 0.085*\"forget\" + 0.085*\"smile\" + 0.085*\"time\"'),\n",
            " (18, '0.134*\"film\" + 0.068*\"ever\" + 0.068*\"see\" + 0.068*\"good\"'),\n",
            " (19, '0.061*\"peopl\" + 0.061*\"cheap\" + 0.061*\"perform\" + 0.061*\"know\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x3I3dBrEOq"
      },
      "source": [
        "1.Features used for Topic Modelling\n",
        "1.Tokenizing the sentences into list of words and removing the stopwords, unnecesarry characters if any.\n",
        "2.Gensim's Phrases Model can implement the bigrams,trigrams,quadgrams and more.The two important argument for the phrases are min_count and threshold.If the threshold is higher we get fewer phrases.\n",
        "3.The two main inputs of the LDA topic model are dictionary and the corpus.Gensim creates a unique id for each word in the document.The produced corpus shown above is a mapping of(word_id,word_frequency).\n",
        "4.LDA MODEL:After creating the dictionary and corpus,we need to provide the num_topics as well.\n",
        "I used chucksize and passes while building LDA model using gensims.Chunksize decides how many documents are processed at a time in the training algorithm and passes decides how often we can train the model on the entire corpus.\n",
        "5.The above LDA model is build with the top 10 topics where each topic is the combination of keywords and each keyword contributes a certain weightage to the topic.\n",
        "lda_model.print_topics(num_words=4) is used to print the keywords of each topic and the importance(weightage) of each keyword with 4 topic terms for each topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwnpczpmrG_5"
      },
      "source": [
        "Top 20 Clusters for topic Modeling\n",
        "The top 20 clusters obtained are:\n",
        "[(0, '0.490*\"good\" + 0.073*\"thing\" + 0.073*\"astonish\" + 0.003*\"far\"'),\n",
        " (1, '0.125*\"good\" + 0.064*\"first\" + 0.064*\"act\" + 0.064*\"see\"'),\n",
        " (2, '0.143*\"rather\" + 0.143*\"superhero\" + 0.007*\"brain\" + 0.007*\"rate\"'),\n",
        " (3, '0.154*\"bad\" + 0.079*\"suspen\" + 0.079*\"hour\" + 0.079*\"good\"'),\n",
        " (4, '0.143*\"extrem\" + 0.143*\"overr\" + 0.073*\"sooner\" + 0.073*\"rate\"'),\n",
        " (5, '0.125*\"joke\" + 0.064*\"broken\" + 0.064*\"requiem\" + 0.064*\"peopl\"'),\n",
        " (6, '0.126*\"everyon\" + 0.126*\"cinemat\" + 0.126*\"seem\" + 0.006*\"rate\"'),\n",
        " (7, '0.165*\"review\" + 0.008*\"astonish\" + 0.008*\"feel\" + 0.008*\"rate\"'),\n",
        " (8, '0.112*\"actor\" + 0.112*\"final\" + 0.112*\"real\" + 0.112*\"movi\"'),\n",
        " (9, '0.247*\"hype\" + 0.166*\"believ\" + 0.085*\"want\" + 0.085*\"go\"'),\n",
        " (10, '0.199*\"real\" + 0.133*\"life\" + 0.068*\"movi\" + 0.068*\"joker\"'),\n",
        " (11, '0.102*\"good\" + 0.101*\"book\" + 0.101*\"sheep\" + 0.101*\"comic\"'),\n",
        " (12, '0.073*\"feed\" + 0.073*\"faux\" + 0.073*\"fuel\" + 0.073*\"would\"'),\n",
        " (13, '0.101*\"exist\" + 0.101*\"ten\" + 0.101*\"asleep\" + 0.101*\"fall\"'),\n",
        " (14, '0.112*\"everyon\" + 0.112*\"feel\" + 0.112*\"brain\" + 0.112*\"wash\"'),\n",
        " (15, '0.084*\"realli\" + 0.043*\"understand\" + 0.043*\"go\" + 0.043*\"perfect\"'),\n",
        " (16, '0.070*\"see\" + 0.070*\"make\" + 0.070*\"account\" + 0.036*\"tonight\"'),\n",
        " (17, '0.085*\"watch\" + 0.085*\"forget\" + 0.085*\"smile\" + 0.085*\"time\"'),\n",
        " (18, '0.134*\"film\" + 0.068*\"ever\" + 0.068*\"see\" + 0.068*\"good\"'),\n",
        " (19, '0.061*\"peopl\" + 0.061*\"cheap\" + 0.061*\"perform\" + 0.061*\"know\"')]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67sa5_nHroil"
      },
      "source": [
        "Summarize and describe the topics for each cluster\n",
        "0.Joker is a real hype with every perfect aspect.\n",
        "1.Spend good time and best movie making.\n",
        "2.Everyone feel like broken and brain wash.\n",
        "3.Bad film and insult to best comic book.\n",
        "4.Psychological study rather than a superhero.\n",
        "5.Over extreme performance.\n",
        "6.Saw tonight and made an account to tell everyone about faux drama.\n",
        "7.One act performance of people I have ever seen.\n",
        "8.Believe the hype,Good film to watch.\n",
        "9.Went blind dint enjoy that life of joker really disappointed.\n",
        "10.Just amazing. How does this movie exist.\n",
        "11.I feel like everyone is brain washed.\n",
        "12.Only certain people can relate.\n",
        "13.Kind of a letdown.\n",
        "14.Best dc movie since the dark knight Rises\n",
        "15.Joaquin oscar joker is best Dark suspense thriller. Darker than dark knight.\n",
        "16.Stop comparing it with Endgame\n",
        "17.Perfect in every aspect.\n",
        "18.I don't get it\n",
        "19.The hype is real"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Yelp Review Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.\n",
        "\n",
        "The data can be download from Dropbox: https://www.dropbox.com/s/59hsrk56sfwh9u2/Assignment%20four%20data%20Yelp%20%28question%201%20and%202%29.zip?dl=0 \n",
        "\n",
        "The data was saved in json format, here is an example of the data (for this task, you only need to use the star rating and the review text fields):\n",
        "\n",
        "{\n",
        "    // string, 22 character unique review id\n",
        "    \"review_id\": \"zdSx_SD6obEhz9VrW9uAWA\",\n",
        "\n",
        "    // string, 22 character unique user id, maps to the user in user.json\n",
        "    \"user_id\": \"Ha3iJu77CxlrFm-vQRs_8g\",\n",
        "\n",
        "    // string, 22 character business id, maps to business in business.json\n",
        "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\",\n",
        "\n",
        "    // integer, star rating\n",
        "    \"stars\": 4,\n",
        "\n",
        "    // string, date formatted YYYY-MM-DD\n",
        "    \"date\": \"2016-03-09\",\n",
        "\n",
        "    // string, the review itself\n",
        "    \"text\": \"Great place to hang out after work: the prices are decent, and the ambience is fun. It's a bit loud, but very lively. The staff is friendly, and the food is good. They have a good selection of drinks.\",\n",
        "\n",
        "    // integer, number of useful votes received\n",
        "    \"useful\": 0,\n",
        "\n",
        "    // integer, number of funny votes received\n",
        "    \"funny\": 0,\n",
        "\n",
        "    // integer, number of cool votes received\n",
        "    \"cool\": 0\n",
        "}\n",
        "\n",
        "The sentiment of can be accessed based on the star rating, if no star information avaliable for a record, just remove that record. Detail star and sentiment level can be matched blew:\n",
        "\n",
        "Very positive = 5 stars\n",
        "\n",
        "Positive = 4 stars\n",
        "\n",
        "Neutral = 3 stars\n",
        "\n",
        "Negative = 2 stars\n",
        "\n",
        "Very negative = 1 star\n",
        "\n",
        "Here is code for yelp data preprocessing: https://github.com/Yelp/dataset-examples. \n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features (tf-idf, sentiment lexicon, word2vec, etc). Considering achieve the best performance as you can. \n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. \n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "df = pd.read_csv('Assignment4_File_5731.csv',encoding = \"ISO-8859-1\")\n",
        "X = df['Clean_Title']\n",
        "y = df['Sentiment']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huTkL64ussrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7429f18-32df-4b5d-b672-28b47d7e3f28"
      },
      "source": [
        "#LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
        "\n",
        "vect = CountVectorizer(min_df=2, ngram_range=(1, 2))\n",
        "X_train = vect.fit(X_train).transform(X_train) \n",
        "X_test = vect.transform(X_test)\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train,y_train)\n",
        "y_pred=logreg.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred,average = 'macro'))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred,average = 'macro'))\n",
        "print(\"f1 score:\",metrics.f1_score(y_test, y_pred,average = 'macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7\n",
            "Precision: 0.47222222222222215\n",
            "Recall: 0.48888888888888893\n",
            "f1 score: 0.47771836007130125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z4ZA6ZCsySj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5c6a4b-3946-4c67-f755-64d64740e892"
      },
      "source": [
        "#support Vector Machine\n",
        "from sklearn import svm\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
        "vect = CountVectorizer(min_df=2, ngram_range=(1, 2))\n",
        "X_train = vect.fit(X_train).transform(X_train) \n",
        "X_test = vect.transform(X_test)\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred,average = 'macro'))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred,average = 'macro'))\n",
        "print(\"f1 score:\",metrics.f1_score(y_test, y_pred,average = 'macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n",
            "Precision: 0.3790849673202614\n",
            "Recall: 0.36296296296296293\n",
            "f1 score: 0.30769230769230765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2vWU5nBs6d0"
      },
      "source": [
        "Features used for Sentiment Classification A feature or aspect is an attribute of entity.Different features can generate different sentiment analysis. The features used for sentiment classification are clean titles of the joker movie reviews and sentiment i.e. if the title is positive,negative or neutral.I chose these features to obtain the better accuarcy because the accuracy depends on the relatinship between the features.\n",
        "\n",
        "For feature selection we need to divide the dataset into dependent and independent variables.Here dependent is the sentiment columns and independent is the clean title from our dataset.\n",
        "\n",
        "Some of the techniques used for feature selection are Mutual Information,Chi-Square,Information gain and TF-idf to select geatures from high dimentionality of feature set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62m2GMjKtCyK"
      },
      "source": [
        "1.LogisticRegression Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification.It describes and estimates the relationship between one dependent binary variable and independent variables.\n",
        "1.I have dividied the columns into two types variables dependent(y) and independent variable(X). 2.splitting the dataset into 80% training data and 20% test data using function train_test_split().In this function we need to pass three parameters dependent variable,independent variable and test_size and random_state can be used to select the records randomly. 3.LogisticRegression() function is used to create a Logistic Regression classifier object. 4.Fit() function is used to fit the model on the train set and predict() function is used to make predictions on the test set. 5.Accurcay,Precision,recall and f1 score are calculated to evaluate the metrics.\n",
        "2.Support Vector Machine\n",
        "SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes. 1.I have dividied the columns into two types variables dependent(y) and independent variable(X). 2.splitting the dataset into 80% training data and 20% test data using function train_test_split().In this function we need to pass three parameters dependent variable,independent variable and test_size and random_state can be used to select the records randomly. 3.SVC() function is used to create a Support Vector classifier object by passing an argument kernel as the linear kernel.A linear Kernel is nothing but a dot product of any two given observations. 4.Fit() function is used to fit the model on the train set and predict() function is used to make predictions on the test set. 5.Accurcay,Precision,recall and f1 score are calculated to evaluate the metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDsrxZ6itEus"
      },
      "source": [
        "Comparing the algorithms based on accuracy,precision,recall amd f1 score\n",
        "\n",
        "I have chosen the logistic regression and support vector machine algorithms because our dataset is very small and for accurate results over a small dataset we have few algorithms like logistic regression, SVM using linear, Naive Bayes, RandomForest.\n",
        "\n",
        "From the baove results based on the accuracy,precision,recall and f1 score we can say that logistic regression classifier is best suited for this dataset as the accuracy is 70% which is good enough and the f1 score is close to 0.5 which means the algorithm is good for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from Dropbox: https://www.dropbox.com/s/52j9hpxppfo921o/assignment4-question3-data.zip?dl=0. Here is an axample for the implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg6Vmjs0u9MJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "6fc74d85-729f-4aaa-e263-da47da5a0ec2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "train_X = train.drop('SalePrice', axis=1)\n",
        "train_y = train.SalePrice\n",
        "test_X = test\n",
        "onehot_train_X = pd.get_dummies(train_X)\n",
        "onehot_test_X = pd.get_dummies(test_X)\n",
        "train_X, test_X = onehot_train_X.align(onehot_test_X, join='left', axis=1)\n",
        "my_imputer = SimpleImputer()\n",
        "train_X = my_imputer.fit_transform(train_X)\n",
        "test_X = my_imputer.transform(test_X)\n",
        "reg = LinearRegression()\n",
        "cv_scores = cross_val_score(reg, train_X, train_y, cv=5)\n",
        "reg = LinearRegression()\n",
        "reg.fit(train_X, train_y)\n",
        "predictions = reg.predict(test_X)\n",
        "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice':predictions})\n",
        "my_submission.to_csv(\"Predicted_House_Price.csv\")\n",
        "my_submission"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1461</td>\n",
              "      <td>112709.903813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1462</td>\n",
              "      <td>161010.363863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1463</td>\n",
              "      <td>186961.179807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1464</td>\n",
              "      <td>197300.132022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1465</td>\n",
              "      <td>205541.292943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>2915</td>\n",
              "      <td>86095.528874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>2916</td>\n",
              "      <td>81610.279372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>2917</td>\n",
              "      <td>181080.339264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>2918</td>\n",
              "      <td>117021.339522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>2919</td>\n",
              "      <td>225002.291367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1459 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Id      SalePrice\n",
              "0     1461  112709.903813\n",
              "1     1462  161010.363863\n",
              "2     1463  186961.179807\n",
              "3     1464  197300.132022\n",
              "4     1465  205541.292943\n",
              "...    ...            ...\n",
              "1454  2915   86095.528874\n",
              "1455  2916   81610.279372\n",
              "1456  2917  181080.339264\n",
              "1457  2918  117021.339522\n",
              "1458  2919  225002.291367\n",
              "\n",
              "[1459 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4idH4huVvJtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe102745-c13f-4ddf-f887-2b5513a1b49f"
      },
      "source": [
        "\n",
        "#Performance evaluation for the baove model.\n",
        "reg = LinearRegression()\n",
        "reg.fit(train_X,train_y)\n",
        "X_train , X_test, Y_train, Y_test = train_test_split(\n",
        "    train_X,train_y,\n",
        "        test_size=0.20,\n",
        "        random_state=42)\n",
        "predictions = reg.predict(X_test)\n",
        "print(\"Mean Absolute Error\", metrics.mean_absolute_error(Y_test, predictions))\n",
        "print(\"Mean Square Error\",metrics.mean_squared_error(Y_test, predictions))\n",
        "print(\"R2 Score\",metrics.r2_score(Y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error 14426.740961107349\n",
            "Mean Square Error 522409274.01144296\n",
            "R2 Score 0.931892169915545\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}