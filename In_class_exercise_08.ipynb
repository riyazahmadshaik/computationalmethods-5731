{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "In_class_exercise_08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyazahmadshaik/computationalmethods-5731/blob/master/In_class_exercise_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X8ryO5G00zU"
      },
      "source": [
        "# **The eighth in-class-exercise (20 points in total, 10/29/2020)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74PaL7W700zW"
      },
      "source": [
        "The data for this exercise is from the dataset you created from assignment three. Please perform answer the following questions based on your data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT21K6dr00zb"
      },
      "source": [
        "## (1) (10 points) Write a python program to extract the sentiment related terms from the corpus. You may use python package such as polyglot or external lexicon resources in the question. Rank the sentiment related terms by frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbQ9YiiT1p--",
        "outputId": "ae7eb505-0d56-4f33-f5d3-4ab106e7557c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "\n",
        "df = pd.read_csv(\"/content/SentimentReview.csv\")\n",
        "\n",
        "df['Content_Text'] = df['Content_Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in string.punctuation))\n",
        "df['Content_Text'] = df['Content_Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df['Content_Text'] = df['Content_Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "print(df['Content_Text'].head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "0                                      amaz movi exist\n",
            "1                               went second time watch\n",
            "2               psycholog studi rather superhero flick\n",
            "3    joaquin oscar joker best dark suspens thriller...\n",
            "4                                         venic review\n",
            "Name: Content_Text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWh9A-xp5MFD",
        "outputId": "d2df44cd-d298-44a4-c918-43b4a4b2949a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words=[]\n",
        "for i in df['Content_Text']:\n",
        "  words.append(nltk.word_tokenize(i))\n",
        "print(words[:5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['amaz', 'movi', 'exist'], ['went', 'second', 'time', 'watch'], ['psycholog', 'studi', 'rather', 'superhero', 'flick'], ['joaquin', 'oscar', 'joker', 'best', 'dark', 'suspens', 'thriller', 'darker', 'dark', 'knight'], ['venic', 'review']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCDLjbI-7Gt5",
        "outputId": "2fdde3ca-3fe3-40d7-a8cd-20c3cf209047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc =[]\n",
        "for i in range(len(df)-1):\n",
        "  sentiment = (df['Sentiment'])[i]\n",
        "  text = nltk.word_tokenize((df['Content_Text'])[i])\n",
        "  doc.append((text,sentiment))\n",
        "\n",
        "print(doc[:1])\n",
        "import random\n",
        "random.shuffle(doc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['amaz', 'movi', 'exist'], 'Positive')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkZmJ8gc8Uva",
        "outputId": "2607c277-f8ba-4218-cb83-a47ff000086d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "total_words=[]\n",
        "for w in range(len(words)):\n",
        "  total = nltk.FreqDist(words[w])\n",
        "  total_words.append(list(total))\n",
        "flat_list = [item for sublist in total_words for item in sublist]\n",
        "flat_list[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amaz',\n",
              " 'movi',\n",
              " 'exist',\n",
              " 'went',\n",
              " 'second',\n",
              " 'time',\n",
              " 'watch',\n",
              " 'psycholog',\n",
              " 'studi',\n",
              " 'rather',\n",
              " 'superhero',\n",
              " 'flick',\n",
              " 'joaquin',\n",
              " 'oscar',\n",
              " 'joker',\n",
              " 'best',\n",
              " 'dark',\n",
              " 'suspens',\n",
              " 'thriller',\n",
              " 'darker']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9cLjVmA-8bB"
      },
      "source": [
        "def document_features(doc):\n",
        "  doc_words=set(doc)\n",
        "  features={}\n",
        "  for w in flat_list:\n",
        "    features['contains({})'.format(w)] = (w in doc_words)\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpMBTmb4_d6O"
      },
      "source": [
        "feature_sets = [(document_features(d),c) for (d,c) in doc]\n",
        "train_data,test_data = feature_sets[:80],feature_sets[80:]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXs49YH2BXRA",
        "outputId": "c8a819f4-31a3-4473-cd85-27b423336a1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(nltk.classify.accuracy(classifier,test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5789473684210527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfezjqXiBfeL",
        "outputId": "f1b565d7-8df5-404e-b759-ebef7b84f577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "          contains(film) = True           Neutra : Positi =      7.3 : 1.0\n",
            "        contains(review) = True           Neutra : Positi =      4.4 : 1.0\n",
            "        contains(endgam) = True           Neutra : Positi =      4.4 : 1.0\n",
            "         contains(great) = True           Neutra : Positi =      4.4 : 1.0\n",
            "       contains(everyon) = True           Neutra : Positi =      4.4 : 1.0\n",
            "          contains(best) = True           Positi : Negati =      4.2 : 1.0\n",
            "          contains(real) = True           Neutra : Negati =      2.9 : 1.0\n",
            "    contains(masterpiec) = True           Positi : Negati =      2.9 : 1.0\n",
            "           contains(act) = True           Neutra : Positi =      2.6 : 1.0\n",
            "          contains(dark) = True           Positi : Negati =      2.0 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdn9JAZ700zk"
      },
      "source": [
        "## (2) (10 points) Compare the performance of the following tools in sentiment identification: TextBlob (https://textblob.readthedocs.io/en/dev/), VADER (https://github.com/cjhutto/vaderSentiment), TFIDF-based Support Vector Machine (SVM) (Split your data into training and testing data). Take your own annotation as the standard answers. \n",
        "\n",
        "Reference code: https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhdJHEeo2qad",
        "outputId": "c1fd3267-dc11-45dc-cbf4-07a6ceaf0ffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#textblob\n",
        "import textblob\n",
        "from textblob import TextBlob\n",
        "for i in df['Content_Text'][:20]:\n",
        "  sent = TextBlob(i)\n",
        "  print(sent.sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.2333333333333333, subjectivity=0.3666666666666667)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.1, subjectivity=0.65)\n",
            "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.2, subjectivity=0.30000000000000004)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=-0.5, subjectivity=0.2)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=1.0, subjectivity=0.3)\n",
            "Sentiment(polarity=0.3, subjectivity=0.1)\n",
            "Sentiment(polarity=0.5, subjectivity=0.5)\n",
            "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n",
            "Sentiment(polarity=0.0, subjectivity=0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61N-rLqxCf_Y",
        "outputId": "b57dca2a-8b70-4aae-f081-df5a652e0147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#vader\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "for i in df['Content_Text'][:20]:\n",
        "  print(analyzer.polarity_scores(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 0.496, 'pos': 0.504, 'compound': 0.7269}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 0.256, 'pos': 0.744, 'compound': 0.4404}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.128}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5994}\n",
            "{'neg': 0.0, 'neu': 0.4, 'pos': 0.6, 'compound': 0.128}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.6369}\n",
            "{'neg': 0.442, 'neu': 0.209, 'pos': 0.349, 'compound': -0.1139}\n",
            "{'neg': 0.0, 'neu': 0.312, 'pos': 0.688, 'compound': 0.296}\n",
            "{'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmF8C01IDJnJ",
        "outputId": "04a96369-6f3f-44c4-be00-0cee6326c8d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tv = TfidfVectorizer(\n",
        "                    ngram_range = (1,3),\n",
        "                    sublinear_tf = True,\n",
        "                    max_features = 40000)\n",
        "X_train=df[:70]\n",
        "X_test=df[70:]\n",
        "train_tv = tv.fit_transform(X_train['Content_Text'])\n",
        "test_tv = tv.transform(X_test['Content_Text'])\n",
        "from sklearn import svm\n",
        "classifier_svm = svm.SVC(kernel='linear')\n",
        "classifier_svm.fit(train_tv,X_train['Sentiment'])\n",
        "predict = classifier_svm.predict(test_tv)\n",
        "\n",
        "import sklearn\n",
        "print(sklearn.metrics.accuracy_score(X_test['Sentiment'],predict))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klE-JqdLRgv3"
      },
      "source": [
        "Vader: Used Vader for first 20 sentences. We can see from the result all the sentences all the neutral. We can see positive value is (0.504). So we believe vader would be accurate than textblob.\n",
        "\n",
        "SVM:  We can see that the precision value is more for positive values and f-score of 0.5666666666666667. So, I can say positive values are accurate. So, I can say it is also accurate for upto postive values.\n",
        "\n",
        "The number of positive sentences are 233, number of negative sentences are 366 and neutral are 210. That means, we can say that number of positives are more. \n",
        "In conclusion, I can say that vadar is better than SVM and Textblob. vadar is better when compared to SVM."
      ]
    }
  ]
}